<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hang Li</title>
  
  <meta name="author" content="Hang Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-487QF3B8KF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-487QF3B8KF');
</script>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hang Li</name>
              </p>
              <p>
                I am a PhD student advised by <a href="https://www.dbs.ifi.lmu.de/~tresp/">Prof. Dr. Volker Tresp</a> at <a href="https://www.lmu.de/en/">LMU Munich</a> and <a href="https://www.siemens.com/global/en.html">Siemens AG</a>. 
                Prior to that, I received my Master's degree in computer science at <a href="https://www.tum.de/en/">Technical University of Munich</a>, supervised by <a href="https://www.dbs.ifi.lmu.de/~tresp/">Prof. Dr. Volker Tresp</a> and <a href="https://www.in.tum.de/en/daml/team/damlguennemann/">Prof. Dr. Stephan Günnemann</a>.
              </p>
              <p>
              </p>
              <p style="text-align:center">
                <a href="mailto:bitlihang@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/HangLi-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.de/citations?user=gZrfKq0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hangligit">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/HangLi11">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/hang-li-b8419b161/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/HangLi.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/HangLi.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interest</heading>
              <p>
                My research fouces on multimodal learning, including representational alignment in vision-language models, generative foundation models, and the explainability of vision-language models with scene graphs and visual grounding. 
                In my current project, I explore the interpretable latent space of text-to-image models to align them for responsible and fair image generation.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="dalle_stop()" onmouseover="dalle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dalle_image'>
                  <img src='images/dalle_before.jpg' width="200"></div>
                <img src='images/dalle_before.jpg' width="200">
              </div>
              <script type="text/javascript">
                function dalle_start() {
                  document.getElementById('dalle_image').style.opacity = "1";
                }

                function dalle_stop() {
                  document.getElementById('dalle_image').style.opacity = "0";
                }
                dalle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2212.12249">
                <papertitle>Do DALL-E and Flamingo Understand Each Other?</papertitle>
              </a>
              <br>
              <strong>Hang Li*</strong>,
              <a href="https://jindonggu.github.io/">Jindong Gu*</a>,
              <a href="https://www.linkedin.com/in/rajat-koner/">Rajat Koner</a>,
              <a href="https://www.linkedin.com/in/sahandsharifzadeh/?locale=de_DE">Sahand Sharifzadeh</a>,
              <a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a>
              <br>
							<em>ICCV</em>, 2023
              <br>
              <a href="https://dalleflamingo.github.io/">code</a> /
              <a href="https://arxiv.org/abs/2212.12249">arXiv</a> /
              <a href="https://www.jiqizhixin.com/articles/2023-01-08-2">cn blog</a>
              <p></p>
              <p>
                We explore two types of large-scale multimodal generative models, image-to-text and text-to-image. The image-to-text model generates abstract descriptions of an image, whereas the text-to-image model decodes the text into low-level visual pixel features. These two models are closely related but their relationship is little understood. In this work, we study if large multimodal generative models understand each other. Specifically, if Flamingo describes an image in text, can DALLE reconstruct an image similar to the input image from the text?
              </p>
            </td>
          </tr>


          <tr onmouseout="concept_stop()" onmouseover="concept_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='concept_image'>
                  <img src='images/Self_discovery.jpg' width="200"></div>
                <img src='images/Self_discovery.jpg' width="200">
              </div>
              <script type="text/javascript">
                function concept_start() {
                  document.getElementById('concept_image').style.opacity = "1";
                }

                function concept_stop() {
                  document.getElementById('concept_image').style.opacity = "0";
                }
                concept_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.17216">
                <papertitle>Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation</papertitle>
              </a>
              <br>
              <strong>Hang Li</strong>,
              <a href="https://www.linkedin.com/in/chengzhi-shen/?originalSubdomain=de/">Chengzhi Shen</a>,
              <a href="https://www.robots.ox.ac.uk/~phst/">Philip Torr</a>,
              <a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a>,
              <a href="https://jindonggu.github.io/">Jindong Gu</a>
              <br>
							<em>Preprint</em>, 2023
              <br>
              <!-- <a href="">code (Coming Soon)</a> / -->
              <a href="https://arxiv.org/abs/2311.17216">arXiv</a>
              <p></p>
              <p>
              Previous work interprets vectors in an interpretable latent space of diffusion models as semantic concepts. However, existing approaches cannot discover directions for arbitrary concepts, such as those related to inappropriate concepts. In this work, we propose a novel self-supervised approach to find interpretable latent directions for a given concept. With the discovered vectors, we further propose a simple approach to mitigate inappropriate generation.
              </p>
            </td>
          </tr>


          <tr onmouseout="comp_stop()" onmouseover="comp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='comp_image'>
                  <img src='images/comp_before.jpg' width="200"></div>
                <img src='images/comp_before.jpg' width="200">
              </div>
              <script type="text/javascript">
                function comp_start() {
                  document.getElementById('comp_image').style.opacity = "1";
                }

                function comp_stop() {
                  document.getElementById('comp_image').style.opacity = "0";
                }
                comp_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.03964">
                <papertitle>Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining</papertitle>
              </a>
              <br>
              <a href="https://github.com/ugorsahin">Ugur Sahin*</a>,
              <strong>Hang Li*</strong>,
              <a href="https://vision.in.tum.de/members/khamuham">Qadeer Khan</a>,
              <a href="https://vision.in.tum.de/members/cremers">Daniel Cremers</a>,
              <a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a>
              <br>
							<em>WACV</em>, 2024, to appear
              <br>
              <a href="https://">code</a> /
              <a href="https://">arXiv</a>
              <p></p>
              <p>
              Leveraging generative hard negative samples, we significantly enhance VLMs' performance in tasks involving multimodal compositional reasoning.
              </p>
            </td>
          </tr>


          <tr onmouseout="pathfinder_stop()" onmouseover="pathfinder_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pathfinder_image'>
                  <img src='images/pathfinder.png' width="200"></div>
                <img src='images/pathfinder.png' width="200">
              </div>
              <script type="text/javascript">
                function pathfinder_start() {
                  document.getElementById('pathfinder_image').style.opacity = "1";
                }

                function pathfinder_stop() {
                  document.getElementById('pathfinder_image').style.opacity = "0";
                }
                pathfinder_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-88361-4_7?error=cookies_not_supported&code=d786d7d0-c29d-4a97-a1f8-0d33f064917c">
                <papertitle>Biologically Inspired Neural Path Finding</papertitle>
              </a>
              <br>

              <strong>Hang Li*</strong>,
              <a href="https://vision.in.tum.de/members/khamuham">Qadeer Khan*</a>,
              <a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a>,
              <a href="https://vision.in.tum.de/members/cremers">Daniel Cremers</a>
              <br>
							<em>Brain Informatics</em>, 2022
              <br>
              <a href="https://github.com/hangligit/pathfinding/">code</a> /
              <a href="https://arxiv.org/pdf/2206.05971.pdf">arXiv</a>
              <p></p>
              <p>
                In this paper, we take inspiration from attributes of the brain, to develop a computational framework to find the optimal low cost path between a source node and a destination node in a generalized graph.
              </p>
            </td>
          </tr>


          <tr onmouseout="tensor_stop()" onmouseover="tensor_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tensor_image'>
                  <img src='images/tensor_after.png' width="200"></div>
                <img src='images/tensor_before.png' width="200">
              </div>
              <script type="text/javascript">
                function tensor_start() {
                  document.getElementById('tensor_image').style.opacity = "1";
                }

                function tensor_stop() {
                  document.getElementById('tensor_image').style.opacity = "0";
                }
                tensor_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2109.13392v2">
                <papertitle>The Tensor Brain: A Unified Theory of Perception, Memory and Semantic Decoding</papertitle>
              </a>
              <br>

              <a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a>,
              <a href="https://www.linkedin.com/in/sahandsharifzadeh/?locale=de_DE">Sahand Sharifzadeh*</a>,
              <strong>Hang Li*</strong>,
              Dario Konopatzki,
              <a href="https://www.dbs.ifi.lmu.de/cms/personen/mitarbeiter/ma/index.html">Yunpu Ma</a>
              <br>
							<em>Journal of Neural Computation</em>, 2023
              <br>
              <a href="">code</a> /
              <a href="https://arxiv.org/abs/2109.13392v1">arXiv</a>
              <p></p>
              <p>
                We present a unified computational theory of an agent’s perception and memory. Episodic memory and semantic memory evolved as emergent properties in a development to gain a deeper understanding of sensory information, to provide a context, and to provide a sense of the current state of the world.
              </p>
            </td>
          </tr>


          <tr onmouseout="graphhopper_stop()" onmouseover="graphhopper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='graphhopper_image'>
                  <img src='images/sgrvqa_after.png' width="200"></div>
                <img src='images/sgrvqa_after.png' width="200">
              </div>
              <script type="text/javascript">
                function graphhopper_start() {
                  document.getElementById('graphhopper_image').style.opacity = "1";
                }

                function graphhopper_stop() {
                  document.getElementById('graphhopper_image').style.opacity = "0";
                }
                graphhopper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-88361-4_7?error=cookies_not_supported&code=d786d7d0-c29d-4a97-a1f8-0d33f064917c">
                <papertitle>Graphhopper: Multi-hop Scene Graph Reasoning for Visual Question Answering</papertitle>
              </a>
              <br>

              <a href="https://www.linkedin.com/in/rajat-koner/">Rajat Koner*</a>,
              <strong>Hang Li*</strong>,
              <a href="https://www.linkedin.com/in/marcel-hildebrandt-523805114/">Marcel Hildebrandt*</a>,
              <a href="https://www.linkedin.com/in/dipn-ds/">Deepan Das</a>,		    
              <a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a>,
              <a href="https://www.in.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>
              <br>
							<em>ISWC</em>, 2021
              <br>
              <a href="https://github.com/rajatkoner08/Graphhopper/">code</a> /
              <a href="https://arxiv.org/abs/2007.01072v1">arXiv</a>
              <p></p>
              <p>
                We find that Graphhopper outperforms state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin.
              </p>
            </td>
          </tr>

          <tr onmouseout="tensor_stop()" onmouseover="tensor_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sgrvqa_image'>
                  <img src='images/sgrvqa_before.png' width="200"></div>
                <img src='images/sgrvqa_before.png' width="200">
              </div>
              <script type="text/javascript">
                function sgrvqa_start() {
                  document.getElementById('sgrvqa_image').style.opacity = "1";
                }

                function sgrvqa_stop() {
                  document.getElementById('sgrvqa_image').style.opacity = "0";
                }
                sgrvqa_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.01072v1">
                <papertitle>Scene Graph Reasoning for Visual Question Answering</papertitle>
              </a>
              <br>

              <a href="https://www.linkedin.com/in/marcel-hildebrandt-523805114/">Marcel Hildebrandt*</a>,
              <strong>Hang Li*</strong>,
              <a href="https://www.linkedin.com/in/rajat-koner/">Rajat Koner*</a>,
              <a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a>,
              <a href="https://www.in.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>
              <br>
							<em>ICML Workshop</em>, 2020
              <br>
              <a href="https://github.com/rajatkoner08/Graphhopper/">code</a> /
              <a href="https://arxiv.org/abs/2007.01072v1">arXiv</a>
              <p></p>
              <p>
              We propose a novel method that approaches the VQA task by performing context-driven, sequential reasoning based on the objects and their semantic and spatial relationships present in the scene.
              </p>
            </td>
          </tr>


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last updated: 30 Nov 2023
                <br>
                Website template from <a href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
